{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4 - RNN sentence classifer .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CzRZe-VIfjwm3g-W-3f_29gU1LhlLPpc",
      "authorship_tag": "ABX9TyMSfeof5AJKh3zQ3xQJEqqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nightted/ML-LeeHongYi-HW/blob/master/HW4_RNN_sentence_classifer_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do0xvZWyyqDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "51be80b2-8846-4c69-b071-32872ec9c8bb"
      },
      "source": [
        "!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8\n",
            "To: /content/data.zip\n",
            "45.1MB [00:00, 109MB/s]\n",
            "Archive:  data.zip\n",
            "replace training_label.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: training_label.txt      \n",
            "  inflating: testing_data.txt        \n",
            "  inflating: training_nolabel.txt    \n",
            "data.zip  sample_data\t    training_label.txt\n",
            "drive\t  testing_data.txt  training_nolabel.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mj9j-jy1RG9",
        "colab_type": "text"
      },
      "source": [
        "# Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmYmOgkHzEjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from gensim.models import word2vec\n",
        " \n",
        "# Data path\n",
        "path_train_l = '/content/training_label.txt'\n",
        "path_train_ul = '/content/training_nolabel.txt'\n",
        "path_train_t = '/content/testing_data.txt'\n",
        "w2v_path = '/content/drive/My Drive/Colab Notebooks/W2V_model.model'\n",
        " \n",
        "class PreProcessData():\n",
        " \n",
        "  def __init__(self,path_train_l,path_train_ul,path_train_t,w2v_path=w2v_path):\n",
        " \n",
        "    self.path_train_l = path_train_l\n",
        "    self.path_train_ul = path_train_ul\n",
        "    self.path_train_t = path_train_t\n",
        "    self.w2v_path = w2v_path\n",
        "    self.W2V_model = None\n",
        " \n",
        "    # Loading the data \n",
        "    self.train_label_x , self.train_label_y = self.read_train_data(path_train_l)\n",
        "    self.train_unlabel_x =  self.read_train_data(path_train_ul)\n",
        "    self.test_x = self.read_train_data(path_train_t)\n",
        " \n",
        "    # Check the w2v model exsit or not , if not , keep training .\n",
        "    if  self.w2v_path != None :  \n",
        "      self.W2V_model = word2vec.Word2Vec.load(self.w2v_path)\n",
        "    else :\n",
        "      self.Training_word2vec()\n",
        "      self.w2v_path = './W2V_model.model'\n",
        "      self.W2V_model = word2vec.Word2Vec.load(self.w2v_path)\n",
        " \n",
        "    # Extracting the word list and embedding ,matrix\n",
        "    self.wordtoidx , self.idxtoword , self.Embedding_matrix = self.Preprocess_extractmatrix(self.W2V_model)\n",
        "    print(\"!!!Data processing finish!!!\")\n",
        " \n",
        "  #fix the abbreviation like was't or i've ....\n",
        "  def fix_the_abbreviation(self,data):\n",
        "    \n",
        "    for ele in data:\n",
        "      if ele == \"'\":\n",
        "        idx = data.index(ele)\n",
        "        data[idx-1:idx+2] = [''.join(data[idx-1:idx+2])]  \n",
        "    return data\n",
        " \n",
        "  # loading and pre-processing data\n",
        "  def read_train_data(self,path,train_label = False,test_label=False):\n",
        " \n",
        "    train_label = True if path.split('.')[0].split('_')[1] == 'label' else False\n",
        "    test_label = True if path.split('.')[0].split('_')[1] == 'data' else False\n",
        " \n",
        "    if train_label:\n",
        " \n",
        "      with open(path) as f :\n",
        "        dataX = [self.fix_the_abbreviation(line.strip().split(' ')[2:]) for line in f ]\n",
        "        f.seek(0) # https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file\n",
        "        dataY = [line.strip().split(' ')[0] for line in f ]\n",
        " \n",
        "      return dataX ,dataY\n",
        "    \n",
        "    elif test_label :\n",
        " \n",
        "      with open(path) as f :\n",
        "        next(f)\n",
        "        dataX = [self.fix_the_abbreviation(line.strip()[2:].split(' ')) for line in f ] \n",
        " \n",
        "      return dataX\n",
        " \n",
        "    else:\n",
        " \n",
        "      with open(path) as f :\n",
        "        dataX = [self.fix_the_abbreviation(line.strip().split(' ')) for line in f ] \n",
        "      return dataX\n",
        "  \n",
        "  #Training W2V model \n",
        "  def Training_word2vec(self,Total_TrainingData=None):\n",
        " \n",
        "    if Total_TrainingData == None:\n",
        "      Total_TrainingData = self.train_label_x + self.train_unlabel_x\n",
        " \n",
        "    # W2V parameters\n",
        "    size = 250 # the dimension of embedding layer\n",
        "    window = 5  # the window to grab the word before & after the center word\n",
        "    min_count = 5 # Only collect the words appears > min_count  \n",
        "    workers = 12 # multi-thread in training\n",
        "    iter = 10 # the number of epoch , is sensitive to the result?\n",
        "    sg = 1 # 0: CBOW , 1: Skip-Gram\n",
        "    hs = 0 # 0: Negative sampling(Logistic) ,1: Softmax\n",
        "    negative = 4 # the sample amount in negative sampling\n",
        " \n",
        "    print(\"Training start!!!!!\")\n",
        "    W2V_model = word2vec.Word2Vec(Total_TrainingData , size = size , negative = negative , window = window , min_count = min_count, workers = workers , iter = iter , sg = sg, hs = hs) \n",
        "    W2V_model.save('./W2V_model.model')\n",
        "    print(\"Training Finish!!!!!\")\n",
        " \n",
        "  #Extract the word list and word embedding matrix \n",
        "  def Preprocess_extractmatrix(self,model):\n",
        " \n",
        "    wordtoidx = {}\n",
        "    idxtoword = []\n",
        "    Embedding_matrix = []\n",
        " \n",
        "    for i , word in enumerate(model.wv.vocab):   \n",
        " \n",
        "      wordtoidx[word] = i\n",
        "      idxtoword.append(word)\n",
        "      Embedding_matrix.append(model[word])\n",
        "    \n",
        "    Embedding_matrix = torch.tensor(Embedding_matrix) # transfer the embed matrix to tensor in torch\n",
        "    Result = self.Add_word(model= model,wordtoidx=wordtoidx,idxtoword=idxtoword,Embedding_matrix=Embedding_matrix,added_words=['<UNK>','<PAD>']) \n",
        " \n",
        "    return Result\n",
        " \n",
        "  # Adding the <unknown> , <Padding> word into the word library of embedding \n",
        "  def Add_word(self,model,wordtoidx,idxtoword,Embedding_matrix,added_words):\n",
        " \n",
        "      # add <UNK> and <PAD> into sequence\n",
        "    for added_word in added_words:\n",
        " \n",
        "      wordtoidx[added_word] = len(wordtoidx)\n",
        "      idxtoword.append(added_word)\n",
        "      \n",
        "      #Adding the <PAD> <UNK> to embedding matrix\n",
        "      Embedding_dim = model.vector_size\n",
        "      cat_vector = torch.empty(1,Embedding_dim)\n",
        "      torch.nn.init.normal_(cat_vector) \n",
        "      Embedding_matrix = torch.cat((Embedding_matrix,cat_vector),0)\n",
        " \n",
        "    return wordtoidx , idxtoword ,Embedding_matrix\n",
        " \n",
        "  #Transfer the words in sentence into word vector\n",
        "  def sen_to_vec(self,data):\n",
        " \n",
        "    all_sen = []\n",
        "    for i , sen in enumerate(data):\n",
        "      #print('We are now dealing sentence {}'.format(i))\n",
        "      sub_sen = []\n",
        "      for word in sen:\n",
        "        if word not in self.wordtoidx:\n",
        "          sub_sen.append(self.wordtoidx['<UNK>'])\n",
        "        else:\n",
        "          sub_sen.append(self.wordtoidx[word])\n",
        " \n",
        "      all_sen.append(sub_sen)\n",
        "    all_sen = self.pad_sen_blank(all_sen)\n",
        " \n",
        "    return torch.LongTensor(all_sen) #training data 記得轉換成 long tensor type!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        " \n",
        "  # Cut the word whose length of sentence is larger than 20 , fill the blank to <PAD> for whose length of sentence is less than 20\n",
        "  def pad_sen_blank(self,all_sen,standard_len = 15):\n",
        " \n",
        "    for sen in all_sen:\n",
        "      \n",
        "      if len(sen) >= standard_len:\n",
        "        sen[:] = sen[:standard_len]\n",
        "      else:\n",
        "        sen[standard_len:] =  [self.wordtoidx[\"<PAD>\"]] * (standard_len - len(sen))\n",
        "    \n",
        "    return all_sen\n",
        "  \n",
        "  #Return the Word vect data\n",
        "  def Return_data(self):\n",
        " \n",
        "    VecData_label_x = self.sen_to_vec(self.train_label_x)\n",
        "    VecData_unlabel_x = self.sen_to_vec(self.train_unlabel_x)\n",
        "    VecData_test_x = self.sen_to_vec(self.test_x)\n",
        "    Label_y = torch.LongTensor([int(labels) for labels in self.train_label_y])\n",
        " \n",
        "    return VecData_label_x , VecData_unlabel_x , VecData_test_x  , Label_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h5T40GFwSbS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5cc681e0-949e-4d23-df82-9e39069b7355"
      },
      "source": [
        "Preprocess = PreProcessData(path_train_l,path_train_ul,path_train_t ,w2v_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training start!!!!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Finish!!!!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:111: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "!!!Data processing finish!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCe3hrQXz9mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VecData_label_x , VecData_unlabel_x , VecData_test_x , Label_y  = Preprocess.Return_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R8JxRvAziJL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85623d35-40f5-4be0-f6e1-b43138dd5a3a"
      },
      "source": [
        "Embedding_Matrix = Preprocess.Embedding_matrix\n",
        "Embedding_Matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([52483, 250])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4so-JsYa14yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        " \n",
        "class RNN_dataset(data.Dataset):\n",
        " \n",
        "  def __init__(self,X,Y=None):\n",
        " \n",
        "    self.x = X\n",
        "    self.y = Y\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        " \n",
        "    if self.y == None:\n",
        "      return self.x[idx]\n",
        " \n",
        "    else : \n",
        "      return self.x[idx] , self.y[idx]\n",
        "    \n",
        "  def __len__(self):\n",
        "    \n",
        "    return len(self.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_bSyFj-3ZOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8jQMOOe45iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define LSTM Neural Network\n",
        " \n",
        "class LSTM(nn.Module):\n",
        " \n",
        "  def __init__(self, embedding_dim , hidden_dim , embedding_matrix ,num_layers , dropout):\n",
        "    super(LSTM,self).__init__()\n",
        "    # Embedding layer\n",
        "    self.embedding_matrix = embedding_matrix  # Note that this is type of TORCH TENSOR !!!!!!\n",
        "    self.embedding = nn.Embedding(self.embedding_matrix.shape[0],self.embedding_matrix.shape[1]) # initialize embedding function\n",
        "    self.embedding.weight = nn.Parameter(self.embedding_matrix) # Loading the parameter from embedding matrix\n",
        "    self.embedding.weight.requires_grad = False # fix the params of embedding layer in the network\n",
        "    #LSTM layer\n",
        "    self.lstm = nn.LSTM(embedding_dim , hidden_dim ,num_layers ,batch_first = True)\n",
        "    #Classifier layer\n",
        "    self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
        "                                     nn.Linear(hidden_dim,1),\n",
        "                                     nn.Sigmoid())\n",
        "    \n",
        "  def forward(self, x ):\n",
        " \n",
        "    word_embed = self.embedding(x)\n",
        "    hidden , h_c = self.lstm(word_embed, None)\n",
        "    output = self.classifier(h_c[0])  #Extract the last hidden layer output\n",
        " \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVIhVmjRc5p2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "2e289389-8418-4895-c02d-592c8f7c8333"
      },
      "source": [
        "# Training\n",
        " \n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "import torch.nn.functional as F\n",
        " \n",
        "#Important!!!!!!!!!! Set Cuda !!!!!!!!!!!!!!!!!\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model params\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "num_workers = 8\n",
        "epoch = 20\n",
        " \n",
        "#split train , validate data\n",
        "Tx,Ty = VecData_label_x[20000:] , Label_y[20000:]\n",
        "Vx,Vy = VecData_label_x[:20000] , Label_y[:20000]\n",
        " \n",
        "Train_set = RNN_dataset(Tx,Ty)\n",
        "Validate_set= RNN_dataset(Vx,Vy)\n",
        "Trainloader = data.DataLoader(Train_set, batch_size = batch_size, shuffle = True )\n",
        "ValidateLoader =  data.DataLoader(Validate_set, batch_size = batch_size, shuffle = True  )\n",
        " \n",
        "model = LSTM(embedding_dim =250 , hidden_dim = 150 , embedding_matrix = Embedding_Matrix ,num_layers = 1, dropout = 0.5)\n",
        "model.cuda()\n",
        "#別把model.train放這邊ㄚㄚㄚ!!! 要放 train loop 裡面, 因為裡面會不斷在 eval() 跟 train() 之間轉換!!!\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr = lr)\n",
        "\n",
        "Train_loss = []\n",
        "Train_acc = []\n",
        "Validate_loss = []\n",
        "Validate_acc = []\n",
        "# training step\n",
        "for epochs in range(epoch):\n",
        " \n",
        "  model.train()\n",
        " \n",
        "  train_loss , train_acc = 0 , 0\n",
        "  for times , datas in enumerate(Trainloader):\n",
        "    ###################################################\n",
        "    data_x = datas[0].to(device , dtype = torch.long)\n",
        "    data_y = datas[1].to(device , dtype = torch.float)\n",
        "    ###################################################\n",
        "    pred_y = model(data_x)\n",
        "    pred_y = pred_y.squeeze()\n",
        "    loss = criterion(pred_y,data_y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        " \n",
        "  vali_loss , vali_acc = 0 , 0  \n",
        "  #evaluate the validate data \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        " \n",
        "    for times , train_datas in enumerate(Trainloader):\n",
        " \n",
        "      data_x = train_datas[0].to(device , dtype = torch.long)\n",
        "      data_y = train_datas[1].to(device , dtype = torch.float)\n",
        "      pred_y = model(data_x)\n",
        "      pred_y = pred_y.squeeze() # 去除掉dim =1的dimension\n",
        "      loss = criterion(pred_y,data_y)   \n",
        "      train_loss += loss\n",
        "      train_acc += np.sum(np.around(pred_y.cpu().numpy()) == data_y.cpu().numpy())\n",
        "      #print(train_loss,train_acc,(times+1)*128)\n",
        " \n",
        "    Train_loss.append(train_loss/Train_set.__len__())\n",
        "    Train_acc.append(train_acc/Train_set.__len__()*100)\n",
        "    \n",
        " \n",
        "    for times , vali_datas in enumerate(ValidateLoader):\n",
        " \n",
        "      data_x = vali_datas[0].to(device , dtype = torch.long)\n",
        "      data_y = vali_datas[1].to(device , dtype = torch.float)\n",
        "      pred_y = model(data_x)\n",
        "      pred_y = pred_y.squeeze()\n",
        "      loss = criterion(pred_y,data_y)   \n",
        "      vali_loss += loss\n",
        "      vali_acc += np.sum(np.around(pred_y.cpu().numpy()) == data_y.cpu().numpy())\n",
        "      #print(vali_loss,vali_acc,(times+1)*128)\n",
        " \n",
        "    Validate_loss.append(vali_loss/Validate_set.__len__())\n",
        "    Validate_acc.append(vali_acc/Validate_set.__len__()*100)\n",
        "    print(\"In EPOCH {} :\".format(epochs),\"Train_loss:\",Train_loss[-1],\"Train_acc:\",Train_acc[-1],\"Vali_loss:\",Validate_loss[-1],\"Vali_acc:\",Validate_acc[-1],\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In EPOCH 0 : Train_loss: tensor(0.0036, device='cuda:0') Train_acc: 78.32277777777777 Vali_loss: tensor(0.0036, device='cuda:0') Vali_acc: 77.815 %\n",
            "In EPOCH 1 : Train_loss: tensor(0.0035, device='cuda:0') Train_acc: 78.7761111111111 Vali_loss: tensor(0.0035, device='cuda:0') Vali_acc: 78.05 %\n",
            "In EPOCH 2 : Train_loss: tensor(0.0033, device='cuda:0') Train_acc: 80.2461111111111 Vali_loss: tensor(0.0034, device='cuda:0') Vali_acc: 79.38 %\n",
            "In EPOCH 3 : Train_loss: tensor(0.0032, device='cuda:0') Train_acc: 80.77888888888889 Vali_loss: tensor(0.0034, device='cuda:0') Vali_acc: 79.21000000000001 %\n",
            "In EPOCH 4 : Train_loss: tensor(0.0031, device='cuda:0') Train_acc: 81.68444444444445 Vali_loss: tensor(0.0034, device='cuda:0') Vali_acc: 79.845 %\n",
            "In EPOCH 5 : Train_loss: tensor(0.0030, device='cuda:0') Train_acc: 82.46222222222222 Vali_loss: tensor(0.0034, device='cuda:0') Vali_acc: 80.01 %\n",
            "In EPOCH 6 : Train_loss: tensor(0.0029, device='cuda:0') Train_acc: 83.17722222222223 Vali_loss: tensor(0.0034, device='cuda:0') Vali_acc: 80.075 %\n",
            "In EPOCH 7 : Train_loss: tensor(0.0027, device='cuda:0') Train_acc: 84.30444444444444 Vali_loss: tensor(0.0035, device='cuda:0') Vali_acc: 80.08 %\n",
            "In EPOCH 8 : Train_loss: tensor(0.0026, device='cuda:0') Train_acc: 84.81222222222222 Vali_loss: tensor(0.0036, device='cuda:0') Vali_acc: 79.24 %\n",
            "In EPOCH 9 : Train_loss: tensor(0.0024, device='cuda:0') Train_acc: 86.71611111111112 Vali_loss: tensor(0.0036, device='cuda:0') Vali_acc: 79.325 %\n",
            "In EPOCH 10 : Train_loss: tensor(0.0022, device='cuda:0') Train_acc: 87.965 Vali_loss: tensor(0.0037, device='cuda:0') Vali_acc: 79.27499999999999 %\n",
            "In EPOCH 11 : Train_loss: tensor(0.0020, device='cuda:0') Train_acc: 89.23055555555555 Vali_loss: tensor(0.0041, device='cuda:0') Vali_acc: 78.755 %\n",
            "In EPOCH 12 : Train_loss: tensor(0.0018, device='cuda:0') Train_acc: 90.615 Vali_loss: tensor(0.0042, device='cuda:0') Vali_acc: 78.16 %\n",
            "In EPOCH 13 : Train_loss: tensor(0.0017, device='cuda:0') Train_acc: 91.69555555555556 Vali_loss: tensor(0.0043, device='cuda:0') Vali_acc: 78.135 %\n",
            "In EPOCH 14 : Train_loss: tensor(0.0015, device='cuda:0') Train_acc: 92.40388888888889 Vali_loss: tensor(0.0047, device='cuda:0') Vali_acc: 77.775 %\n",
            "In EPOCH 15 : Train_loss: tensor(0.0014, device='cuda:0') Train_acc: 93.50666666666667 Vali_loss: tensor(0.0049, device='cuda:0') Vali_acc: 77.55499999999999 %\n",
            "In EPOCH 16 : Train_loss: tensor(0.0012, device='cuda:0') Train_acc: 94.21333333333334 Vali_loss: tensor(0.0054, device='cuda:0') Vali_acc: 77.315 %\n",
            "In EPOCH 17 : Train_loss: tensor(0.0011, device='cuda:0') Train_acc: 94.60888888888888 Vali_loss: tensor(0.0056, device='cuda:0') Vali_acc: 77.08500000000001 %\n",
            "In EPOCH 18 : Train_loss: tensor(0.0010, device='cuda:0') Train_acc: 95.27944444444445 Vali_loss: tensor(0.0059, device='cuda:0') Vali_acc: 76.09 %\n",
            "In EPOCH 19 : Train_loss: tensor(0.0009, device='cuda:0') Train_acc: 95.54833333333333 Vali_loss: tensor(0.0062, device='cuda:0') Vali_acc: 76.755 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fJu2Ardy9IR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIh1x7L5BiCP",
        "colab_type": "text"
      },
      "source": [
        "# 總結目前遇到的幾個問題與結論:\n",
        "1. data type LongTensor 與一般 tensor 差異在哪? 何時需要轉換成 LongTensor ??\n",
        "2. 在 evaluation 的時候 Trainloader 會有找不到的問題??\n",
        "3. model.train() 的位置要放對 , 因為eval()也在Loop內,故model.train()也需要在Loop內\n",
        "4. 盡量在第一行就定義 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") , 後續再用 data.to(device) transfer to cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZkLjwnaoqF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "f16154fc-4adc-4f65-84f0-f18eb1325f60"
      },
      "source": [
        " class Robot():\n",
        " \n",
        "    __counter = 0\n",
        " \n",
        "    \n",
        "    def __init__(self):\n",
        "        self.foo()\n",
        " \n",
        "    def boo(self):\n",
        "        print('boo')\n",
        " \n",
        "    @classmethod\n",
        "    def foo(cls):\n",
        "        Robot.__counter+=1\n",
        "        print('foo!',Robot.__counter)\n",
        " \n",
        "    @classmethod\n",
        "    def RobotInstances(cls):\n",
        "        cls.boo(cls)\n",
        "        cls.foo()\n",
        "        return cls\n",
        " \n",
        "Robot.RobotInstances()\n",
        "x=Robot()\n",
        "Robot.RobotInstances()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "boo\n",
            "foo! 1\n",
            "foo! 2\n",
            "boo\n",
            "foo! 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Robot"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}