{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data input and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X_train_fpath = './data/X_train'\n",
    "Y_train_fpath = './data/Y_train'\n",
    "X_test_fpath = './data/X_test'\n",
    "output_fpath = './output_{}.csv'\n",
    "\n",
    "# Totally 54256 (number of data) X 511 (number of feat.)\n",
    "# Parse csv files to numpy array\n",
    "with open(X_train_fpath) as f:\n",
    "    # next() 的原因是不需要 feature 那行 \n",
    "    next(f)\n",
    "    #一行代表一個 data , strip 掉換行符號後,再把每個 data的 feature 弄成 list\n",
    "    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
    "with open(Y_train_fpath) as f:\n",
    "    next(f)\n",
    "    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n",
    "with open(X_test_fpath) as f:\n",
    "    next(f)\n",
    "    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
    "    \n",
    "\n",
    "X_mean = np.mean(X_train[:,:],axis=0).reshape(1,-1) #reshpae(1,-1) 代表 row 搞成一列 , column -1 的意思是 column 數自動補齊計算.\n",
    "X_std = np.std(X_train[:,:],axis=0).reshape(1,-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate_shape: (5426, 510) (5426,) Training_shape: (48830, 510) (48830,)\n"
     ]
    }
   ],
   "source": [
    "Split_ratio = 0.1\n",
    "Validate_len = int(np.round(Split_ratio*X_train.shape[0]))\n",
    "\n",
    "X_validate = X_train[:Validate_len,:]\n",
    "Y_validate = Y_train[:Validate_len]\n",
    "X_training = X_train[Validate_len:,:]\n",
    "Y_training = Y_train[Validate_len:]\n",
    "\n",
    "print (\"Validate_shape:\",X_validate.shape,Y_validate.shape,\"Training_shape:\",X_training.shape,Y_training.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48830, 510) (5426, 510) (27622, 510)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\New\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "X_training = (X_training[:,:] - X_mean) / (X_std+1e-8)\n",
    "X_validate = (X_validate[:,:] - X_mean) / (X_std+1e-8)\n",
    "X_test = (X_test[:,:] - X_mean) / X_std # test data must normalized with the mean & std of training data\n",
    "print(X_training.shape,X_validate.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data:48830\n",
      "Size of validate data:5426\n",
      "Size of testing data:27622\n",
      "Data dimension:510\n"
     ]
    }
   ],
   "source": [
    "train_size = X_training.shape[0]\n",
    "Validate_size = X_validate.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "data_dim = X_train.shape[1]\n",
    "\n",
    "print('Size of training data:{}'.format(train_size))\n",
    "print('Size of validate data:{}'.format(Validate_size))\n",
    "print('Size of testing data:{}'.format(test_size))\n",
    "print('Data dimension:{}'.format(data_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#Randomize the priority of the data\n",
    "def shuffle(X_data,Y_data):\n",
    "    \n",
    "    data_len = X_data.shape[0]\n",
    "    random_pri = []\n",
    "    li = list(range(data_len))\n",
    "    \n",
    "    while len(li)>0 :\n",
    "        \n",
    "        select = random.randint(0,len(li)-1)\n",
    "        random_pri.append(li.pop(select))\n",
    "    \n",
    "    X_data_new = np.zeros((X_data.shape[0],X_data.shape[1]))\n",
    "    Y_data_new = np.zeros(Y_data.shape[0])\n",
    "    \n",
    "    for i in range(len(random_pri)):\n",
    "        \n",
    "            X_data_new[i] = X_data[random_pri[i]]\n",
    "            Y_data_new[i] = Y_data[random_pri[i]]\n",
    "            \n",
    "    return X_data_new , Y_data_new\n",
    "\n",
    "#Calculate Cross Entropy of all data\n",
    "def CrossEntorpy(y_train,y_predict):\n",
    "    \n",
    "    return -y_train@np.log(y_predict)-(1-y_train)@np.log(1-y_predict)\n",
    "\n",
    "def sigmoid(z) :\n",
    "    \n",
    "    return np.clip(1/(1+np.exp(-z)),1e-8,1-1e-8)\n",
    "\n",
    "def sigmoid_linearinput(X,w,b):\n",
    "    \n",
    "    linear = X@w+b\n",
    "    return sigmoid(linear)\n",
    "\n",
    "def gradient(y_hat,X,w,b):\n",
    "    \n",
    "    y_pred = sigmoid_linearinput(X,w,b)\n",
    "    return -1*X.T@(y_hat - y_pred) , -1*np.sum(y_hat - y_pred)\n",
    "\n",
    "#Calculate the accuracy of model \n",
    "def accuracy(y_train,y_pred):\n",
    "    \n",
    "    return (1-sum(np.round(np.abs(y_pred-y_train)))/y_train.shape)*100\n",
    "\n",
    "#The main training process\n",
    "def trianing(epoch, Y_training, Y_validate, X_training, X_validate, w=None, b=None, lr = 0.002, mini_batch_size = 1000):\n",
    "    \n",
    "    # accuracy & loss collecting\n",
    "    Training_accuracy = []\n",
    "    Training_loss = []\n",
    "    Validate_accuracy = []\n",
    "    Validate_loss = []\n",
    "    \n",
    "    #parameter w,b initialize\n",
    "    w = np.zeros(X_training.shape[1])\n",
    "    b = np.zeros(1)\n",
    "    G_w = np.zeros(X_training.shape[1])\n",
    "    G_b = np.zeros(1)\n",
    "    eps = 1e-8\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        \n",
    "        shuffle(X_training,Y_training) # Shuffle the training data to the increase the variation of mini-batch\n",
    "        \n",
    "        for batch_count in range(int(np.floor(X_training.shape[0]/mini_batch_size))+1):\n",
    "            \n",
    "            X = X_training[batch_count*mini_batch_size:(batch_count+1)*mini_batch_size]\n",
    "            Y = Y_training[batch_count*mini_batch_size:(batch_count+1)*mini_batch_size]\n",
    "            \n",
    "            #print()\n",
    "            #print(batch_count,np.floor(X_training.shape[0]/mini_batch_size),X.shape,Y.shape)\n",
    "            #Using Adagrad optimizer\n",
    "            w_grad , b_grad = gradient(Y,X,w,b)\n",
    "            G_w += w_grad**2\n",
    "            G_b += b_grad**2\n",
    "            \n",
    "            w = w - lr/(np.sqrt(G_w+eps))*w_grad\n",
    "            b = b - lr/(np.sqrt(G_b+eps))*b_grad\n",
    "        \n",
    "        # Collect the accuracy & loss of \"training\" data\n",
    "        #print(Y_training ,sigmoid_linearinput(X_training,w,b),sum(np.round(np.abs(sigmoid_linearinput(X_training,w,b)-Y_training ))))\n",
    "        Train_loss = CrossEntorpy(Y_training , sigmoid_linearinput(X_training,w,b))/X_training.shape[0]\n",
    "        Train_accu = accuracy(Y_training , sigmoid_linearinput(X_training,w,b))\n",
    "        Training_loss.append(Train_loss)\n",
    "        Training_accuracy.append(Train_accu) \n",
    "        \n",
    "        # Collect the accuracy & loss of \"Validate\" data\n",
    "        Valid_loss = CrossEntorpy(Y_validate , sigmoid_linearinput(X_validate,w,b))/X_validate.shape[0]\n",
    "        Valid_accu = accuracy(Y_validate , sigmoid_linearinput(X_validate,w,b))\n",
    "        Validate_loss.append(Valid_loss)\n",
    "        Validate_accuracy.append(Valid_accu)\n",
    "        \n",
    "        print(\"Epoch:\",i,\"Training_loss:\",Train_loss,\"Training_accuracy:\",Train_accu,\"Validate_loss:\",Valid_loss,\"Validate_accuracy:\",Valid_accu)\n",
    "        \n",
    "    return w , b\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training_loss: 0.5984313682173168 Training_accuracy: [70.84783944] Validate_loss: 0.5950621473034323 Validate_accuracy: [71.43383708]\n",
      "Epoch: 1 Training_loss: 0.5782915035077285 Training_accuracy: [72.74626254] Validate_loss: 0.5745570403006697 Validate_accuracy: [73.29524512]\n",
      "Epoch: 2 Training_loss: 0.5664343575261304 Training_accuracy: [73.85214008] Validate_loss: 0.5625368459951716 Validate_accuracy: [74.58532989]\n",
      "Epoch: 3 Training_loss: 0.5580443322848533 Training_accuracy: [74.67335654] Validate_loss: 0.5540414875424599 Validate_accuracy: [75.41467011]\n",
      "Epoch: 4 Training_loss: 0.5515565349449641 Training_accuracy: [75.22015155] Validate_loss: 0.5474697327296255 Validate_accuracy: [75.98599337]\n",
      "Epoch: 5 Training_loss: 0.5462653001823712 Training_accuracy: [75.67888593] Validate_loss: 0.5421041696229181 Validate_accuracy: [76.40987836]\n",
      "Epoch: 6 Training_loss: 0.5417934469168163 Training_accuracy: [76.0208888] Validate_loss: 0.5375632093469993 Validate_accuracy: [76.96277184]\n",
      "Epoch: 7 Training_loss: 0.5379165245727849 Training_accuracy: [76.28302273] Validate_loss: 0.5336205399332422 Validate_accuracy: [77.25764836]\n",
      "Epoch: 8 Training_loss: 0.5344905571636414 Training_accuracy: [76.5287733] Validate_loss: 0.5301313095287256 Validate_accuracy: [77.46037597]\n",
      "Epoch: 9 Training_loss: 0.5314178866294187 Training_accuracy: [76.73561335] Validate_loss: 0.5269974021688447 Validate_accuracy: [77.79211205]\n",
      "Epoch: 10 Training_loss: 0.5286293412719126 Training_accuracy: [76.98750768] Validate_loss: 0.5241493809706382 Validate_accuracy: [77.93955031]\n",
      "Epoch: 11 Training_loss: 0.5260742013560398 Training_accuracy: [77.26192914] Validate_loss: 0.5215363572423609 Validate_accuracy: [78.12384814]\n",
      "Epoch: 12 Training_loss: 0.5237142087258864 Training_accuracy: [77.44828999] Validate_loss: 0.5191199557040886 Validate_accuracy: [78.30814596]\n",
      "Epoch: 13 Training_loss: 0.521519815226553 Training_accuracy: [77.57935695] Validate_loss: 0.5168705410256101 Validate_accuracy: [78.38186509]\n",
      "Epoch: 14 Training_loss: 0.5194677382116217 Training_accuracy: [77.7759574] Validate_loss: 0.5147647618032827 Validate_accuracy: [78.38186509]\n",
      "Epoch: 15 Training_loss: 0.5175393136947524 Training_accuracy: [77.93364735] Validate_loss: 0.5127838972425164 Validate_accuracy: [78.54773314]\n",
      "Epoch: 16 Training_loss: 0.5157193548317965 Training_accuracy: [78.10362482] Validate_loss: 0.5109127117987416 Validate_accuracy: [78.71360118]\n",
      "Epoch: 17 Training_loss: 0.5139953409861431 Training_accuracy: [78.23673971] Validate_loss: 0.5091386418479589 Validate_accuracy: [78.87946922]\n",
      "Epoch: 18 Training_loss: 0.5123568291575752 Training_accuracy: [78.41490887] Validate_loss: 0.5074512055614082 Validate_accuracy: [78.97161813]\n",
      "Epoch: 19 Training_loss: 0.5107950186639005 Training_accuracy: [78.56031128] Validate_loss: 0.5058415665459758 Validate_accuracy: [78.97161813]\n",
      "Epoch: 20 Training_loss: 0.5093024237338171 Training_accuracy: [78.67909072] Validate_loss: 0.5043022057221538 Validate_accuracy: [79.06376705]\n",
      "Epoch: 21 Training_loss: 0.5078726235458141 Training_accuracy: [78.77534303] Validate_loss: 0.5028266708628287 Validate_accuracy: [79.08219683]\n",
      "Epoch: 22 Training_loss: 0.5065000688041593 Training_accuracy: [78.88183494] Validate_loss: 0.5014093828143934 Validate_accuracy: [79.13748618]\n",
      "Epoch: 23 Training_loss: 0.505179930226151 Training_accuracy: [78.97194348] Validate_loss: 0.5000454837293712 Validate_accuracy: [79.30335422]\n",
      "Epoch: 24 Training_loss: 0.5039079785330709 Training_accuracy: [79.05795617] Validate_loss: 0.49873071687300535 Validate_accuracy: [79.41393292]\n",
      "Epoch: 25 Training_loss: 0.5026804884231046 Training_accuracy: [79.17468769] Validate_loss: 0.49746133046161056 Validate_accuracy: [79.54294139]\n",
      "Epoch: 26 Training_loss: 0.5014941610128225 Training_accuracy: [79.27094] Validate_loss: 0.49623400000502654 Validate_accuracy: [79.69037965]\n",
      "Epoch: 27 Training_loss: 0.5003460606533331 Training_accuracy: [79.38971943] Validate_loss: 0.49504576504923964 Validate_accuracy: [79.81938813]\n",
      "Epoch: 28 Training_loss: 0.49923356304500327 Training_accuracy: [79.46958837] Validate_loss: 0.493893977235884 Validate_accuracy: [79.92996683]\n",
      "Epoch: 29 Training_loss: 0.49815431231395535 Training_accuracy: [79.53102601] Validate_loss: 0.49277625733665875 Validate_accuracy: [79.98525617]\n",
      "Epoch: 30 Training_loss: 0.4971061852570617 Training_accuracy: [79.59451157] Validate_loss: 0.49169045946565915 Validate_accuracy: [80.04054552]\n",
      "Epoch: 31 Training_loss: 0.49608726136629167 Training_accuracy: [79.6743805] Validate_loss: 0.4906346410777624 Validate_accuracy: [80.07740509]\n",
      "Epoch: 32 Training_loss: 0.49509579754681976 Training_accuracy: [79.75015359] Validate_loss: 0.4896070376654927 Validate_accuracy: [80.15112422]\n",
      "Epoch: 33 Training_loss: 0.49413020667352525 Training_accuracy: [79.80544747] Validate_loss: 0.4886060412975837 Validate_accuracy: [80.22484335]\n",
      "Epoch: 34 Training_loss: 0.4931890393067526 Training_accuracy: [79.88122056] Validate_loss: 0.4876301823190603 Validate_accuracy: [80.28013269]\n",
      "Epoch: 35 Training_loss: 0.49227096802418885 Training_accuracy: [79.94061028] Validate_loss: 0.48667811366896 Validate_accuracy: [80.40914117]\n",
      "Epoch: 36 Training_loss: 0.4913747739315403 Training_accuracy: [80.01638337] Validate_loss: 0.48574859737783366 Validate_accuracy: [80.46443052]\n",
      "Epoch: 37 Training_loss: 0.4904993349976037 Training_accuracy: [80.08806062] Validate_loss: 0.4848404928902656 Validate_accuracy: [80.50129008]\n",
      "Epoch: 38 Training_loss: 0.489643615924813 Training_accuracy: [80.14130657] Validate_loss: 0.4839527469232183 Validate_accuracy: [80.51971987]\n",
      "Epoch: 39 Training_loss: 0.4888066593183419 Training_accuracy: [80.18021708] Validate_loss: 0.4830843846231215 Validate_accuracy: [80.55657943]\n",
      "Epoch: 40 Training_loss: 0.48798757795844233 Training_accuracy: [80.23346304] Validate_loss: 0.4822345018262747 Validate_accuracy: [80.63029856]\n",
      "Epoch: 41 Training_loss: 0.4871855480141675 Training_accuracy: [80.28056523] Validate_loss: 0.48140225826064004 Validate_accuracy: [80.64872835]\n",
      "Epoch: 42 Training_loss: 0.48639980306368485 Training_accuracy: [80.36453] Validate_loss: 0.4805868715542041 Validate_accuracy: [80.75930704]\n",
      "Epoch: 43 Training_loss: 0.48562962880840843 Training_accuracy: [80.41982388] Validate_loss: 0.47978761193712705 Validate_accuracy: [80.83302617]\n",
      "Epoch: 44 Training_loss: 0.48487435838618764 Training_accuracy: [80.48740528] Validate_loss: 0.4790037975429165 Validate_accuracy: [80.86988574]\n",
      "Epoch: 45 Training_loss: 0.48413336820357084 Training_accuracy: [80.52631579] Validate_loss: 0.4782347902286609 Validate_accuracy: [80.88831552]\n",
      "Epoch: 46 Training_loss: 0.4834060742193707 Training_accuracy: [80.60208888] Validate_loss: 0.4774799918465807 Validate_accuracy: [80.94360487]\n",
      "Epoch: 47 Training_loss: 0.4826919286218971 Training_accuracy: [80.65328691] Validate_loss: 0.47673884090926943 Validate_accuracy: [80.96203465]\n",
      "Epoch: 48 Training_loss: 0.48199041685063065 Training_accuracy: [80.69629326] Validate_loss: 0.4760108095994419 Validate_accuracy: [81.017324]\n",
      "Epoch: 49 Training_loss: 0.4813010549201762 Training_accuracy: [80.7474913] Validate_loss: 0.4752954010820479 Validate_accuracy: [81.10947291]\n",
      "Epoch: 50 Training_loss: 0.48062338701024715 Training_accuracy: [80.78844972] Validate_loss: 0.47459214708252445 Validate_accuracy: [81.20162182]\n",
      "Epoch: 51 Training_loss: 0.47995698329041175 Training_accuracy: [80.81916854] Validate_loss: 0.47390060569995135 Validate_accuracy: [81.27534095]\n",
      "Epoch: 52 Training_loss: 0.4793014379525578 Training_accuracy: [80.85398321] Validate_loss: 0.4732203594280794 Validate_accuracy: [81.3306303]\n",
      "Epoch: 53 Training_loss: 0.47865636742760087 Training_accuracy: [80.88879787] Validate_loss: 0.4725510133607822 Validate_accuracy: [81.38591965]\n",
      "Epoch: 54 Training_loss: 0.478021408766014 Training_accuracy: [80.94409175] Validate_loss: 0.47189219356152434 Validate_accuracy: [81.44120899]\n",
      "Epoch: 55 Training_loss: 0.47739621816435496 Training_accuracy: [80.98095433] Validate_loss: 0.4712435455790361 Validate_accuracy: [81.47806856]\n",
      "Epoch: 56 Training_loss: 0.4767804696221948 Training_accuracy: [81.03829613] Validate_loss: 0.4706047330936165 Validate_accuracy: [81.53335791]\n",
      "Epoch: 57 Training_loss: 0.4761738537157714 Training_accuracy: [81.08744624] Validate_loss: 0.4699754366803924 Validate_accuracy: [81.58864725]\n",
      "Epoch: 58 Training_loss: 0.47557607647633954 Training_accuracy: [81.12430883] Validate_loss: 0.4693553526775152 Validate_accuracy: [81.58864725]\n",
      "Epoch: 59 Training_loss: 0.4749868583626147 Training_accuracy: [81.16526725] Validate_loss: 0.46874419214870466 Validate_accuracy: [81.62550682]\n",
      "Epoch: 60 Training_loss: 0.47440593331794323 Training_accuracy: [81.18779439] Validate_loss: 0.46814167993077005 Validate_accuracy: [81.6439366]\n",
      "Epoch: 61 Training_loss: 0.47383304790390884 Training_accuracy: [81.22465697] Validate_loss: 0.4675475537578283 Validate_accuracy: [81.69922595]\n",
      "Epoch: 62 Training_loss: 0.47326796050302133 Training_accuracy: [81.26356748] Validate_loss: 0.46696156345485623 Validate_accuracy: [81.69922595]\n",
      "Epoch: 63 Training_loss: 0.4727104405839382 Training_accuracy: [81.33934057] Validate_loss: 0.4663834701940436 Validate_accuracy: [81.69922595]\n",
      "Epoch: 64 Training_loss: 0.4721602680233963 Training_accuracy: [81.3905386] Validate_loss: 0.4658130458081125 Validate_accuracy: [81.7545153]\n",
      "Epoch: 65 Training_loss: 0.4716172324796552 Training_accuracy: [81.43559287] Validate_loss: 0.4652500721554035 Validate_accuracy: [81.79137486]\n",
      "Epoch: 66 Training_loss: 0.4710811328127954 Training_accuracy: [81.46016793] Validate_loss: 0.46469434053207637 Validate_accuracy: [81.82823443]\n",
      "Epoch: 67 Training_loss: 0.4705517765477085 Training_accuracy: [81.47859922] Validate_loss: 0.46414565112725126 Validate_accuracy: [81.82823443]\n",
      "Epoch: 68 Training_loss: 0.4700289793760389 Training_accuracy: [81.48474299] Validate_loss: 0.4636038125173553 Validate_accuracy: [81.86509399]\n",
      "Epoch: 69 Training_loss: 0.4695125646937185 Training_accuracy: [81.52570141] Validate_loss: 0.4630686411962983 Validate_accuracy: [81.88352377]\n",
      "Epoch: 70 Training_loss: 0.4690023631710624 Training_accuracy: [81.54618063] Validate_loss: 0.46253996113845836 Validate_accuracy: [81.90195356]\n",
      "Epoch: 71 Training_loss: 0.4684982123527056 Training_accuracy: [81.60966619] Validate_loss: 0.4620176033917358 Validate_accuracy: [81.93881312]\n",
      "Epoch: 72 Training_loss: 0.4679999562849064 Training_accuracy: [81.63219332] Validate_loss: 0.4615014056982069 Validate_accuracy: [81.97567269]\n",
      "Epoch: 73 Training_loss: 0.46750744516799064 Training_accuracy: [81.65676838] Validate_loss: 0.4609912121401428 Validate_accuracy: [81.97567269]\n",
      "Epoch: 74 Training_loss: 0.46702053503192287 Training_accuracy: [81.69567889] Validate_loss: 0.4604868728093701 Validate_accuracy: [81.99410247]\n",
      "Epoch: 75 Training_loss: 0.4665390874331607 Training_accuracy: [81.70387057] Validate_loss: 0.4599882434981296 Validate_accuracy: [82.03096203]\n",
      "Epoch: 76 Training_loss: 0.4660629691711383 Training_accuracy: [81.71820602] Validate_loss: 0.4594951854097691 Validate_accuracy: [82.08625138]\n",
      "Epoch: 77 Training_loss: 0.4655920520228595 Training_accuracy: [81.74278108] Validate_loss: 0.4590075648877461 Validate_accuracy: [82.08625138]\n",
      "Epoch: 78 Training_loss: 0.4651262124942217 Training_accuracy: [81.7837395] Validate_loss: 0.4585252531615578 Validate_accuracy: [82.12311095]\n",
      "Epoch: 79 Training_loss: 0.46466533158681855 Training_accuracy: [81.82469793] Validate_loss: 0.45804812610833473 Validate_accuracy: [82.21525986]\n",
      "Epoch: 80 Training_loss: 0.46420929457906485 Training_accuracy: [81.85132091] Validate_loss: 0.4575760640289415 Validate_accuracy: [82.23368964]\n",
      "Epoch: 81 Training_loss: 0.46375799082060365 Training_accuracy: [81.88203973] Validate_loss: 0.45710895143753427 Validate_accuracy: [82.28897899]\n",
      "Epoch: 82 Training_loss: 0.46331131353902427 Training_accuracy: [81.89637518] Validate_loss: 0.4566466768636041 Validate_accuracy: [82.30740877]\n",
      "Epoch: 83 Training_loss: 0.4628691596580226 Training_accuracy: [81.91071063] Validate_loss: 0.45618913266562117 Validate_accuracy: [82.30740877]\n",
      "Epoch: 84 Training_loss: 0.4624314296261869 Training_accuracy: [81.93733361] Validate_loss: 0.4557362148554701 Validate_accuracy: [82.32583856]\n",
      "Epoch: 85 Training_loss: 0.4619980272556733 Training_accuracy: [81.96805243] Validate_loss: 0.45528782293292597 Validate_accuracy: [82.36269812]\n",
      "Epoch: 86 Training_loss: 0.46156885957008714 Training_accuracy: [81.99877125] Validate_loss: 0.4548438597294851 Validate_accuracy: [82.39955769]\n",
      "Epoch: 87 Training_loss: 0.46114383666094383 Training_accuracy: [82.01925046] Validate_loss: 0.4544042312609202 Validate_accuracy: [82.39955769]\n",
      "Epoch: 88 Training_loss: 0.4607228715521333 Training_accuracy: [82.0417776] Validate_loss: 0.45396884658797526 Validate_accuracy: [82.45484703]\n",
      "Epoch: 89 Training_loss: 0.4603058800718494 Training_accuracy: [82.06840057] Validate_loss: 0.4535376176846627 Validate_accuracy: [82.47327682]\n",
      "Epoch: 90 Training_loss: 0.4598927807315031 Training_accuracy: [82.09297563] Validate_loss: 0.4531104593136692 Validate_accuracy: [82.56542573]\n",
      "Epoch: 91 Training_loss: 0.459483494611156 Training_accuracy: [82.11345484] Validate_loss: 0.4526872889084066 Validate_accuracy: [82.56542573]\n",
      "Epoch: 92 Training_loss: 0.45907794525105705 Training_accuracy: [82.12983821] Validate_loss: 0.4522680264612895 Validate_accuracy: [82.56542573]\n",
      "Epoch: 93 Training_loss: 0.45867605854889837 Training_accuracy: [82.15236535] Validate_loss: 0.45185259441784176 Validate_accuracy: [82.56542573]\n",
      "Epoch: 94 Training_loss: 0.4582777626624246 Training_accuracy: [82.18308417] Validate_loss: 0.45144091757627186 Validate_accuracy: [82.60228529]\n",
      "Epoch: 95 Training_loss: 0.4578829879170646 Training_accuracy: [82.21175507] Validate_loss: 0.45103292299217496 Validate_accuracy: [82.60228529]\n",
      "Epoch: 96 Training_loss: 0.4574916667182778 Training_accuracy: [82.22609052] Validate_loss: 0.45062853988805324 Validate_accuracy: [82.62071508]\n",
      "Epoch: 97 Training_loss: 0.45710373346832506 Training_accuracy: [82.2629531] Validate_loss: 0.4502276995673611 Validate_accuracy: [82.62071508]\n",
      "Epoch: 98 Training_loss: 0.4567191244871967 Training_accuracy: [82.27114479] Validate_loss: 0.44983033533280115 Validate_accuracy: [82.65757464]\n",
      "Epoch: 99 Training_loss: 0.45633777793745306 Training_accuracy: [82.29571984] Validate_loss: 0.44943638240862793 Validate_accuracy: [82.65757464]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 1.48428895e-01, -1.48252702e-03,  1.31926080e-01, -4.95816872e-02,\n",
       "         9.93718844e-03, -4.04987931e-02, -4.20090152e-03,  4.21593933e-02,\n",
       "         6.82640142e-03, -3.27556999e-02, -3.03038580e-03,  3.36662479e-02,\n",
       "        -1.39522106e-02,  7.35141134e-02,  2.16723419e-02,  3.11962595e-02,\n",
       "         1.93146764e-02,  3.97153284e-03,  7.88060866e-03, -3.54925393e-02,\n",
       "         2.55670097e-02,  3.34545935e-02, -3.29765630e-04,  2.11986473e-02,\n",
       "         4.51960439e-02,  7.14113820e-03, -1.20058883e-02,  2.15634703e-02,\n",
       "        -1.16831322e-02,  1.19459754e-02, -1.18353383e-02, -2.17833804e-02,\n",
       "         9.18925846e-03,  7.76686116e-02, -9.36820047e-03,  4.05385767e-02,\n",
       "         5.84375641e-02, -1.47403514e-02,  4.24084609e-02,  1.32256896e-02,\n",
       "        -1.69746473e-03,  3.92235573e-02,  2.65957488e-02,  2.52174501e-02,\n",
       "        -2.17169669e-02, -2.20080424e-02, -6.81842257e-03,  7.34280416e-03,\n",
       "        -5.16983460e-03, -4.41858560e-03,  2.84910410e-02, -3.99861439e-02,\n",
       "        -8.97079443e-03, -2.58038591e-02, -1.31082140e-02, -2.44361649e-03,\n",
       "        -1.88635320e-02,  2.73454048e-02,  2.10110027e-03, -2.66805401e-03,\n",
       "         5.99748451e-03,  7.29729441e-03, -7.17031982e-02,  1.61339871e-01,\n",
       "        -7.22060382e-02, -1.66852194e-02, -2.19113415e-02,  1.75965182e-02,\n",
       "         2.00092199e-02,  5.22700246e-02, -1.96641517e-02, -6.59599363e-02,\n",
       "        -3.88911994e-02, -7.84970306e-02, -7.29397325e-02, -5.59857256e-02,\n",
       "        -3.35312685e-02, -4.30475179e-02, -2.95043073e-02,  1.09040937e-02,\n",
       "         4.23021395e-02, -3.81566440e-02, -4.96960136e-02, -4.45713627e-02,\n",
       "        -4.38548389e-02, -1.39490849e-02, -1.02133126e-01,  2.05161602e-02,\n",
       "        -4.43886799e-03, -6.31052683e-02, -1.26317803e-02, -7.89463201e-03,\n",
       "         5.16824072e-04, -4.18992023e-02, -4.16415227e-02, -9.32793618e-03,\n",
       "        -4.02318751e-02, -6.03320798e-02,  1.22062676e-01, -1.68702895e-02,\n",
       "        -3.99861439e-02, -1.29429741e-02,  2.28985005e-02,  1.74482348e-01,\n",
       "         1.28211548e-01,  6.64020453e-02,  1.92612441e-03, -1.98643335e-02,\n",
       "         5.99748451e-03, -3.99692655e-02, -3.46601954e-02, -2.90402835e-02,\n",
       "        -9.42558152e-02,  1.68804415e-01, -8.61874733e-02, -7.39592769e-02,\n",
       "         7.12216618e-02,  1.65887210e-01, -3.76316426e-02, -6.02182525e-02,\n",
       "         1.63171818e-01, -6.01210250e-02, -1.54848871e-01,  6.76291650e-04,\n",
       "        -8.16637369e-02, -6.98256176e-02, -2.02040162e-02, -2.06740230e-02,\n",
       "         1.75926713e-03,  2.42037254e-02,  1.24271962e-02,  1.41352543e-02,\n",
       "         2.48037132e-03,  1.27827834e-02,  2.17944282e-02, -3.67346195e-03,\n",
       "        -4.98024721e-02,  2.84910410e-02,  2.65957488e-02,  9.18925846e-03,\n",
       "        -3.54925393e-02,  2.95131816e-02, -2.44361649e-03, -1.20058883e-02,\n",
       "         4.32546729e-02, -3.29765630e-04,  2.55670097e-02,  3.34545935e-02,\n",
       "        -3.99861439e-02, -2.66805401e-03,  2.52174501e-02, -2.20080424e-02,\n",
       "         7.88060866e-03, -7.90392221e-03, -5.16983460e-03,  3.03743925e-02,\n",
       "         7.34280416e-03,  5.37653053e-02, -2.17169669e-02,  9.79087366e-03,\n",
       "         5.99748451e-03, -6.08810188e-02, -2.18165193e-02, -8.90355643e-02,\n",
       "        -1.39490849e-02,  8.20691451e-02, -9.32793618e-03, -2.70251235e-02,\n",
       "         1.89194598e-02, -3.99861439e-02, -5.92788188e-02, -5.53846510e-02,\n",
       "        -6.57660852e-02, -9.77043761e-02,  5.99748451e-03,  9.94977373e-02,\n",
       "         1.97873792e-03,  2.00339694e-02, -2.94616588e-03, -1.39651096e-02,\n",
       "        -2.28245291e-02, -6.94456539e-03, -4.42963333e-03, -9.07315552e-03,\n",
       "        -2.11564799e-02,  1.72460434e-03, -1.03459596e-02, -2.58053189e-02,\n",
       "        -1.77054031e-03, -3.41969253e-03,  1.75416839e-02,  1.09207909e-01,\n",
       "        -1.09207909e-01, -1.74299765e-03,  2.22576900e-02, -7.40393497e-03,\n",
       "        -7.96274585e-03, -1.80738818e-02, -4.36468673e-03,  6.14020240e-03,\n",
       "         6.82640142e-03,  1.72296357e-03, -8.71736888e-03, -1.60947198e-02,\n",
       "        -1.56360571e-02,  2.45399917e-02, -6.51132589e-03,  8.92707554e-03,\n",
       "        -6.85525376e-03, -4.84471346e-03,  2.28356005e-01,  1.65232092e-01,\n",
       "         2.23262222e-01, -3.04217794e-02, -1.36385021e-02, -2.28452046e-02,\n",
       "        -5.42615753e-02,  5.58750557e-02, -2.91427563e-02,  4.98963407e-03,\n",
       "         5.83671646e-03,  3.37322433e-03, -1.63483525e-02,  1.72356684e-03,\n",
       "        -5.04792083e-03,  1.82092539e-02,  1.60635598e-03, -1.20812151e-02,\n",
       "         4.34136393e-03, -4.64842391e-03, -2.12370543e-03, -7.64795231e-03,\n",
       "        -1.00568920e-02, -5.29139535e-03,  1.14880513e-03,  4.99385590e-03,\n",
       "        -1.37889103e-03,  6.20717907e-03,  9.07887121e-03, -2.20589781e-03,\n",
       "        -2.34558060e-03, -1.01522458e-02,  1.54088302e-02, -7.10569227e-03,\n",
       "        -1.08682570e-02, -6.04319084e-03, -6.05275894e-03,  1.48979549e-02,\n",
       "        -5.70903769e-03, -4.29999238e-03, -1.09064857e-04, -2.08026514e-03,\n",
       "        -1.69351159e-03, -2.44552799e-03,  5.55839316e-03,  3.37322433e-03,\n",
       "         2.98078178e-03, -4.55830998e-03, -1.61213740e-03, -6.58169704e-03,\n",
       "        -2.63611375e-03,  4.64809633e-03,  7.29096054e-03, -5.31958515e-03,\n",
       "        -6.35272492e-03, -9.79571136e-03, -2.81187290e-03, -6.27685803e-04,\n",
       "         7.13511935e-03, -5.17630537e-05,  1.03450417e-02,  2.37946773e-03,\n",
       "        -7.18547358e-03, -8.10105297e-03, -1.09754565e-03,  1.03609381e-02,\n",
       "         2.17599500e-03, -5.10600478e-02,  7.46832843e-03, -6.18635459e-03,\n",
       "        -1.44875171e-02,  1.70878222e-02,  1.59825419e-02, -1.65965166e-02,\n",
       "         3.63506734e-03,  8.71134418e-03,  0.00000000e+00, -9.75148190e-03,\n",
       "         6.18872104e-03,  6.01728470e-03,  1.52790598e-03,  3.38100174e-03,\n",
       "         3.52819946e-03,  7.81970489e-04,  2.87903602e-02, -5.60613325e-03,\n",
       "        -6.94541172e-03, -2.98471846e-02, -6.71336244e-03, -9.69141924e-04,\n",
       "         7.16005859e-03,  1.22886212e-02,  0.00000000e+00, -1.47417285e-02,\n",
       "         4.62703695e-03, -1.76151861e-02, -3.81287944e-03,  2.72944737e-03,\n",
       "        -5.72218095e-03,  6.17235537e-03,  3.60924173e-03, -2.87342128e-03,\n",
       "        -4.68934273e-03,  1.95791282e-03, -7.93972025e-03,  7.71984262e-03,\n",
       "        -5.17635976e-02, -2.56571479e-02, -1.31801100e-02, -5.97885996e-04,\n",
       "         2.65399008e-02, -8.95979951e-03, -7.33154570e-03,  3.79369048e-04,\n",
       "         8.05264350e-04,  3.94036822e-03, -1.36612409e-02,  2.84463339e-03,\n",
       "        -1.80245001e-03, -8.76567545e-03, -5.20018129e-03,  7.28102602e-04,\n",
       "        -7.80220573e-03, -2.58009017e-03,  3.79369048e-04,  7.06688176e-03,\n",
       "        -3.05384747e-03,  2.84463339e-03, -2.81148034e-03,  1.72356684e-03,\n",
       "         7.28102602e-04,  1.71162948e-02, -2.58009017e-03,  3.79369048e-04,\n",
       "        -1.60689045e-02, -1.59751275e-03,  2.84463339e-03, -2.81148034e-03,\n",
       "         6.96794246e-03,  1.72356684e-03,  7.28102602e-04,  2.20058517e-04,\n",
       "         2.84463339e-03, -3.37322433e-03,  2.89434584e-03,  7.28102602e-04,\n",
       "        -6.94945392e-03, -1.01902997e-03,  1.38368519e-01, -1.09313374e-02,\n",
       "         9.29511127e-03,  4.38174999e-03,  1.92283958e-03,  5.25264575e-03,\n",
       "         8.22290731e-03,  5.44863893e-04,  1.02536155e-02, -1.78387660e-02,\n",
       "        -3.78673734e-03,  1.94695271e-03,  1.03463392e-03, -6.36892387e-03,\n",
       "         3.64710165e-04,  1.15417750e-02, -2.43332136e-03, -4.29760286e-04,\n",
       "        -4.19164137e-03,  1.05538453e-02,  1.25490770e-04,  4.62483853e-03,\n",
       "        -1.43487485e-04,  5.80612677e-03,  2.54831742e-03, -2.44747503e-03,\n",
       "        -1.99480971e-03, -5.65685386e-03,  1.06256338e-02, -4.52336909e-03,\n",
       "         9.27288586e-03, -7.74204665e-03, -3.68995091e-03,  2.51109436e-02,\n",
       "         6.38575654e-03, -1.06881700e-03,  1.17692994e-02, -2.22948226e-03,\n",
       "         9.85762622e-03, -6.17831592e-04, -3.45491909e-03, -1.40708098e-02,\n",
       "        -2.38276675e-03, -8.27608359e-03, -5.14897842e-03, -3.14939309e-03,\n",
       "         7.04022443e-03, -6.03482311e-03,  5.42205873e-03,  3.88225252e-04,\n",
       "         2.56020787e-03, -1.01773354e-02,  1.20080419e-03, -5.74530448e-03,\n",
       "        -1.09106306e-02,  5.08207042e-03, -2.67557460e-03,  4.84654016e-03,\n",
       "        -1.07809588e-03, -7.45010283e-03,  4.68737373e-03, -3.18150544e-03,\n",
       "        -1.29145267e-02,  3.77368294e-04,  9.57287745e-03,  1.08397139e-02,\n",
       "        -5.34155468e-03, -4.49671359e-03,  1.48258307e-04,  1.50425510e-03,\n",
       "         1.03944816e-03,  2.25373727e-02, -1.26056631e-04,  1.17815627e-02,\n",
       "        -5.42097100e-03,  5.08451538e-03,  2.22869575e-03, -1.25924604e-02,\n",
       "         1.21408553e-02, -1.03184473e-02, -4.42578787e-03,  3.53080146e-03,\n",
       "        -1.02349662e-02, -1.39618687e-02,  1.05802720e-05,  7.61539252e-03,\n",
       "         7.65781583e-03,  2.18375514e-02, -6.97113119e-04, -2.79002844e-03,\n",
       "        -4.22232148e-03,  3.87373182e-03, -1.92201648e-02, -2.18312156e-04,\n",
       "        -4.93774021e-03,  1.07015010e-02,  5.73645625e-03, -3.54859351e-03,\n",
       "         5.31222690e-03,  8.60824410e-03, -1.99695835e-02, -5.50769020e-03,\n",
       "         9.16215347e-03, -7.37233042e-04,  7.48986200e-04,  2.09636543e-03,\n",
       "         1.35184656e-03,  6.32399171e-03, -1.40932373e-02,  1.24263508e-02,\n",
       "        -4.46616884e-03, -1.61226672e-02, -4.35767605e-03,  1.40527983e-03,\n",
       "         2.87153848e-03, -6.93859369e-03,  1.34671812e-02, -4.09799877e-03,\n",
       "        -1.27239733e-03, -3.59735497e-03, -9.71974104e-03,  3.33195529e-03,\n",
       "        -1.16578953e-02,  3.30548272e-03,  1.25114931e-02,  3.03674238e-03,\n",
       "         1.64841218e-02,  1.74895606e-04,  2.25687793e-02, -1.32741284e-02,\n",
       "        -1.03999651e-02,  4.03160999e-03, -6.82896278e-03,  1.90064517e-04,\n",
       "         1.28615425e-04, -6.59614416e-03, -7.08199159e-04,  3.53237032e-03,\n",
       "         8.58486735e-03, -9.34231714e-03, -4.06406508e-02, -7.76982583e-03,\n",
       "         6.66894449e-02, -9.34701345e-03, -7.66693225e-03,  1.42404181e-02,\n",
       "         1.43482927e-03, -2.51477367e-04,  9.34701345e-03,  1.13429699e-01,\n",
       "        -7.28102602e-04,  7.28102602e-04]),\n",
       " array([-0.26238818]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trianing(100, Y_training, Y_validate, X_training, X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
